# Transformers

!SUB

## Transformer

![](images/attention_research_1.png) <!-- .element width="35%" -->

<small>[* Attention Is All You Need](https://arxiv.org/abs/1706.03762)</small>

<small>[* The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer)</small>

!SUB

![](images/transformer_roles.png)<!-- .element width="70%" -->

<small>[* Explainable AI: Visualizing Attention in Transformers - MLOps Community](https://mlops.community/explainable-ai-visualizing-attention-in-transformers/)</small>

!SUB

## Transformer's architecture

![](images/transformer_explained.png)

<small>[* neural networks - Why does transformer has such a complex architecture? - Cross Validated](https://stats.stackexchange.com/questions/512242/why-does-transformer-has-such-a-complex-architecture)</small>

!SUB

## Self attention Quiz

![](images/embedding_apple_quiz_1_1.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

## Self attention Quiz

![](images/embedding_apple_quiz_1_2.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/embedding_apple_quiz_2_1.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/embedding_apple_quiz_2_2.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/bear_honey_context.png)<!-- .element width="90%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

## Feature-based attention

## The Key, Value, and Query

![](images/attention-as-database-query.png)

<small>[* How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)</small>

!SUB

## Self attention

![](images/attention_matrix.png)<!-- .element width="50%" -->

<small>[* Getting Meaning from Text: Self-attention Step-by-step Video](towardsai.net/p/nlp/getting-meaning-from-text-self-attention-step-by-step-video)</small>

!SUB

![](images/transformer_sketch.webp)<!-- .element width="60%" -->

<small>[* Transformer: The Self-Attention Mechanism](https://medium.com/machine-intelligence-and-deep-learning-lab/transformer-the-self-attention-mechanism-d7d853c2c621)</small>

!SUB

##BERTViz - Attention Visualization

![](images/bertviz_head-view.gif)

<small>[* BertViz](https://github.com/jessevig/bertviz?tab=readme-ov-file)</small>

!SUB

## NanoGPT

![](images/nanogpt.jpg)

<small>[* GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs.](https://github.com/karpathy/nanoGPT)</small>

<small>[* NanoGPT: A Small-Scale GPT for Text Generation](https://medium.com/@saipragna.kancheti/nanogpt-a-small-scale-gpt-for-text-generation-in-pytorch-tensorflow-and-jax-641c4efefbd5)</small>
