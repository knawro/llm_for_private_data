# Transformers

!SUB

## Transformer

![](images/attention_research_1.png) <!-- .element width="35%" -->

<small>[* Attention Is All You Need](https://arxiv.org/abs/1706.03762)</small>

<small>[* The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer)</small>

!SUB

![](images/transformer_roles.png)<!-- .element width="70%" -->

<small>[* Explainable AI: Visualizing Attention in Transformers - MLOps Community](https://mlops.community/explainable-ai-visualizing-attention-in-transformers/)</small>

!SUB

## Transformer's architecture

![](images/transformer_explained.png)

<small>[* neural networks - Why does transformer has such a complex architecture? - Cross Validated](https://stats.stackexchange.com/questions/512242/why-does-transformer-has-such-a-complex-architecture)</small>

!SUB

## Feature-based attention

## The Key, Value, and Query

![](images/attention-as-database-query.png)

<small>[* How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)</small>

!SUB

## Self attention

![](images/raschka_self_attention.png)<!-- .element width="75%" -->

<small>[* self-attention-from-scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)</small>

!SUB

## Self attention Quiz

![](images/embedding_apple_quiz_1_1.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

## Self attention Quiz

![](images/embedding_apple_quiz_1_2.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/embedding_apple_quiz_2_1.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/embedding_apple_quiz_2_2.png)<!-- .element width="60%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

!SUB

![](images/bear_honey_context.png)<!-- .element width="90%" -->

<small>[* Serrano Academy](https://www.youtube.com/watch?v=qaWMOYf4ri8)</small>

!SUB

![](images/transformer_explained.png)

<small>[* neural networks - Why does transformer has such a complex architecture? - Cross Validated](https://stats.stackexchange.com/questions/512242/why-does-transformer-has-such-a-complex-architecture)</small>

!SUB

##BERTViz - Attention Visualization

![](images/bertviz_head-view.gif)

<small>[* BertViz](https://github.com/jessevig/bertviz?tab=readme-ov-file)</small>

!SUB
