# PEFT/LoRA/QLoRA

## Parameter-efficient fine-tunning

!SUB

![](images/peft_ft_deci_schemat.png)

<small>[* Full Fine-Tuning, PEFT, Prompt Engineering, or RAG?](https://deci.ai/blog/fine-tuning-peft-prompt-engineering-and-rag-which-one-is-right-for-you/)</small>

!SUB

## Finetuning LLMs Using LoRA

![](images/lora_schemat.jpg)

<small>[* Practical Tips for Finetuning LLMs Using LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)</small>

!SUB

## LoRA: Low-Rank Adaptation of LMM

![](images/lora_decomposition_formula.png)<!-- .element width="40%" -->

<small>[* LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)</small>

!SUB

![](images/lora_decomposition_picture.png)<!-- .element width="40%" -->

<small>[* Fine-Tuning BERT for text classification with LoRA](https://medium.com/@karkar.nizar/fine-tuning-bert-for-text-classification-with-lora-f12af7fa95e4)</small>

!SUB

## Quantizing model parameters

![](images/qlora_schema.jpeg)<!-- .element width="70%" -->

<small>[* QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)</small>

!SUB
##QLoRA

QLoRA uses 4-bit NormalFloat data type

![](images/qlora.webp)<!-- .element width="40%" -->

<small>[* Difference between equally-spaced and equally-sized buckets](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)</small>

!SUB

## Finding your ideal prompt

e.g.

![](images/prompt_testing.png)<!-- .element width="70%" -->

<small>[* together.ai](https://api.together.xyz/playground/chat/meta-llama/Llama-3-70b-chat-hf)</small>

!SUB

# PEFT/LORA/QLORA

## @Athena

!SUB

![](images/litgpt_logo.png)

## Fine-tune with ease

![](images/litgpt_chain.png)

<small>[GitHub - Lightning-AI/litgpt: Pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, ...](https://github.com/Lightning-AI/litgpt/)</small>
